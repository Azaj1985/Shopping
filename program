import pandas as pd
import xgboost as xgb

# Step 1: Load the data
data = pd.read_csv("shopping.csv")

# Step 2: Preprocess the data

# Drop any rows with missing values
data = data.dropna()

# Convert categorical variables to one-hot encoding (e.g., Month, VisitorType)
data = pd.get_dummies(data, columns=["Month", "VisitorType", "Weekend"])

# Split the data into features (X) and the target (y)
X = data.drop(columns=["Revenue"])
y = data["Revenue"]

# Define a custom data split ratio (e.g., 75% for training and 25% for testing)
split_ratio = 0.75
split_index = int(len(data) * split_ratio)

X_train, y_train = X[:split_index], y[:split_index]
X_test, y_test = X[split_index:], y[split_index:]

# Step 3: Train an XGBoost model
model = xgb.XGBClassifier()
model.fit(X_train, y_train)

# Step 4: Make predictions on the testing data
y_pred = model.predict(X_test)

# Step 5: Evaluate the model's performance manually
correct_predictions = 0
true_positives, false_positives, true_negatives, false_negatives = 0, 0, 0, 0

for pred, actual in zip(y_pred, y_test):
    if pred == actual:
        correct_predictions += 1
    if pred and actual:
        true_positives += 1
    elif pred and not actual:
        false_positives += 1
    elif not pred and actual:
        false_negatives += 1
    else:
        true_negatives += 1

total_predictions = len(y_test)

# Calculate metrics
accuracy = correct_predictions / total_predictions
sensitivity = true_positives / (true_positives + false_negatives)  # True Positive Rate
specificity = true_negatives / (true_negatives + false_positives)  # True Negative Rate

print("Accuracy:", accuracy)
print("Sensitivity (True Positive Rate):", sensitivity)
print("Specificity (True Negative Rate):", specificity)
print("Confusion Matrix:")
print(f"True Positives: {true_positives}")
print(f"False Positives: {false_positives}")
print(f"True Negatives: {true_negatives}")
print(f"False Negatives: {false_negatives}")
